{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Fraudulent Firm Prediction - Classification\n",
    "\n",
    "### Author: Ankit Raina\n",
    "### Date of Creation: April 24, 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_risk_data = pd.read_csv('audit_risk.csv')\n",
    "trial_data = pd.read_csv('trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = audit_risk_data.columns.intersection(trial_data.columns).tolist()\n",
    "merged_audit_risk_data = pd.merge(audit_risk_data, trial_data, how = 'inner', left_on=common_columns, right_on=common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector_score</th>\n",
       "      <th>LOCATION_ID</th>\n",
       "      <th>PARA_A</th>\n",
       "      <th>Score_A</th>\n",
       "      <th>Risk_A</th>\n",
       "      <th>PARA_B</th>\n",
       "      <th>Score_B</th>\n",
       "      <th>Risk_B</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>numbers</th>\n",
       "      <th>...</th>\n",
       "      <th>Audit_Risk</th>\n",
       "      <th>Risk</th>\n",
       "      <th>SCORE_A</th>\n",
       "      <th>SCORE_B</th>\n",
       "      <th>Marks</th>\n",
       "      <th>MONEY_Marks</th>\n",
       "      <th>District</th>\n",
       "      <th>Loss</th>\n",
       "      <th>LOSS_SCORE</th>\n",
       "      <th>History_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.89</td>\n",
       "      <td>23</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.508</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>6.68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7148</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.966</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5108</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3096</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.480</td>\n",
       "      <td>10.80</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5060</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2832</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sector_score LOCATION_ID  PARA_A  Score_A  Risk_A  PARA_B  Score_B  Risk_B  \\\n",
       "0          3.89          23    4.18      0.6   2.508    2.50      0.2   0.500   \n",
       "1          3.89           6    0.00      0.2   0.000    4.83      0.2   0.966   \n",
       "2          3.89           6    0.51      0.2   0.102    0.23      0.2   0.046   \n",
       "3          3.89           6    0.00      0.2   0.000   10.80      0.6   6.480   \n",
       "4          3.89           6    0.00      0.2   0.000    0.08      0.2   0.016   \n",
       "\n",
       "   TOTAL  numbers  ...  Audit_Risk  Risk  SCORE_A  SCORE_B  Marks  \\\n",
       "0   6.68      5.0  ...      1.7148     1        6        2      2   \n",
       "1   4.83      5.0  ...      0.5108     0        2        2      2   \n",
       "2   0.74      5.0  ...      0.3096     0        2        2      2   \n",
       "3  10.80      6.0  ...      3.5060     1        2        6      6   \n",
       "4   0.08      5.0  ...      0.2832     0        2        2      2   \n",
       "\n",
       "   MONEY_Marks  District  Loss  LOSS_SCORE  History_score  \n",
       "0            2         2     0           2              2  \n",
       "1            2         2     0           2              2  \n",
       "2            2         2     0           2              2  \n",
       "3            6         2     0           2              2  \n",
       "4            2         2     0           2              2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_audit_risk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 629 entries, 0 to 628\n",
      "Data columns (total 35 columns):\n",
      "Sector_score      629 non-null float64\n",
      "LOCATION_ID       629 non-null object\n",
      "PARA_A            629 non-null float64\n",
      "Score_A           629 non-null float64\n",
      "Risk_A            629 non-null float64\n",
      "PARA_B            629 non-null float64\n",
      "Score_B           629 non-null float64\n",
      "Risk_B            629 non-null float64\n",
      "TOTAL             629 non-null float64\n",
      "numbers           629 non-null float64\n",
      "Score_B.1         629 non-null float64\n",
      "Risk_C            629 non-null float64\n",
      "Money_Value       628 non-null float64\n",
      "Score_MV          629 non-null float64\n",
      "Risk_D            629 non-null float64\n",
      "District_Loss     629 non-null int64\n",
      "PROB              629 non-null float64\n",
      "RiSk_E            629 non-null float64\n",
      "History           629 non-null int64\n",
      "Prob              629 non-null float64\n",
      "Risk_F            629 non-null float64\n",
      "Score             629 non-null float64\n",
      "Inherent_Risk     629 non-null float64\n",
      "CONTROL_RISK      629 non-null float64\n",
      "Detection_Risk    629 non-null float64\n",
      "Audit_Risk        629 non-null float64\n",
      "Risk              629 non-null int64\n",
      "SCORE_A           629 non-null int64\n",
      "SCORE_B           629 non-null int64\n",
      "Marks             629 non-null int64\n",
      "MONEY_Marks       629 non-null int64\n",
      "District          629 non-null int64\n",
      "Loss              629 non-null int64\n",
      "LOSS_SCORE        629 non-null int64\n",
      "History_score     629 non-null int64\n",
      "dtypes: float64(23), int64(11), object(1)\n",
      "memory usage: 176.9+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_audit_risk_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 629 entries, 0 to 628\n",
      "Data columns (total 35 columns):\n",
      "Sector_score      629 non-null float64\n",
      "LOCATION_ID       626 non-null object\n",
      "PARA_A            629 non-null float64\n",
      "Score_A           629 non-null float64\n",
      "Risk_A            629 non-null float64\n",
      "PARA_B            629 non-null float64\n",
      "Score_B           629 non-null float64\n",
      "Risk_B            629 non-null float64\n",
      "TOTAL             629 non-null float64\n",
      "numbers           629 non-null float64\n",
      "Score_B.1         629 non-null float64\n",
      "Risk_C            629 non-null float64\n",
      "Money_Value       628 non-null float64\n",
      "Score_MV          629 non-null float64\n",
      "Risk_D            629 non-null float64\n",
      "District_Loss     629 non-null int64\n",
      "PROB              629 non-null float64\n",
      "RiSk_E            629 non-null float64\n",
      "History           629 non-null int64\n",
      "Prob              629 non-null float64\n",
      "Risk_F            629 non-null float64\n",
      "Score             629 non-null float64\n",
      "Inherent_Risk     629 non-null float64\n",
      "CONTROL_RISK      629 non-null float64\n",
      "Detection_Risk    629 non-null float64\n",
      "Audit_Risk        629 non-null float64\n",
      "Risk              629 non-null int64\n",
      "SCORE_A           629 non-null int64\n",
      "SCORE_B           629 non-null int64\n",
      "Marks             629 non-null int64\n",
      "MONEY_Marks       629 non-null int64\n",
      "District          629 non-null int64\n",
      "Loss              629 non-null int64\n",
      "LOSS_SCORE        629 non-null int64\n",
      "History_score     629 non-null int64\n",
      "dtypes: float64(23), int64(11), object(1)\n",
      "memory usage: 176.9+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_audit_risk_data = merged_audit_risk_data.replace(r'[^\\d.]+',np.nan,regex=True)\n",
    "merged_audit_risk_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sector_score      False\n",
       "LOCATION_ID        True\n",
       "PARA_A            False\n",
       "Score_A           False\n",
       "Risk_A            False\n",
       "PARA_B            False\n",
       "Score_B           False\n",
       "Risk_B            False\n",
       "TOTAL             False\n",
       "numbers           False\n",
       "Score_B.1         False\n",
       "Risk_C            False\n",
       "Money_Value        True\n",
       "Score_MV          False\n",
       "Risk_D            False\n",
       "District_Loss     False\n",
       "PROB              False\n",
       "RiSk_E            False\n",
       "History           False\n",
       "Prob              False\n",
       "Risk_F            False\n",
       "Score             False\n",
       "Inherent_Risk     False\n",
       "CONTROL_RISK      False\n",
       "Detection_Risk    False\n",
       "Audit_Risk        False\n",
       "Risk              False\n",
       "SCORE_A           False\n",
       "SCORE_B           False\n",
       "Marks             False\n",
       "MONEY_Marks       False\n",
       "District          False\n",
       "Loss              False\n",
       "LOSS_SCORE        False\n",
       "History_score     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_audit_risk_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_audit_risk_data = merged_audit_risk_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 625 entries, 0 to 628\n",
      "Data columns (total 35 columns):\n",
      "Sector_score      625 non-null float64\n",
      "LOCATION_ID       625 non-null object\n",
      "PARA_A            625 non-null float64\n",
      "Score_A           625 non-null float64\n",
      "Risk_A            625 non-null float64\n",
      "PARA_B            625 non-null float64\n",
      "Score_B           625 non-null float64\n",
      "Risk_B            625 non-null float64\n",
      "TOTAL             625 non-null float64\n",
      "numbers           625 non-null float64\n",
      "Score_B.1         625 non-null float64\n",
      "Risk_C            625 non-null float64\n",
      "Money_Value       625 non-null float64\n",
      "Score_MV          625 non-null float64\n",
      "Risk_D            625 non-null float64\n",
      "District_Loss     625 non-null int64\n",
      "PROB              625 non-null float64\n",
      "RiSk_E            625 non-null float64\n",
      "History           625 non-null int64\n",
      "Prob              625 non-null float64\n",
      "Risk_F            625 non-null float64\n",
      "Score             625 non-null float64\n",
      "Inherent_Risk     625 non-null float64\n",
      "CONTROL_RISK      625 non-null float64\n",
      "Detection_Risk    625 non-null float64\n",
      "Audit_Risk        625 non-null float64\n",
      "Risk              625 non-null int64\n",
      "SCORE_A           625 non-null int64\n",
      "SCORE_B           625 non-null int64\n",
      "Marks             625 non-null int64\n",
      "MONEY_Marks       625 non-null int64\n",
      "District          625 non-null int64\n",
      "Loss              625 non-null int64\n",
      "LOSS_SCORE        625 non-null int64\n",
      "History_score     625 non-null int64\n",
      "dtypes: float64(23), int64(11), object(1)\n",
      "memory usage: 175.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector_score</th>\n",
       "      <th>PARA_A</th>\n",
       "      <th>Score_A</th>\n",
       "      <th>Risk_A</th>\n",
       "      <th>PARA_B</th>\n",
       "      <th>Score_B</th>\n",
       "      <th>Risk_B</th>\n",
       "      <th>TOTAL</th>\n",
       "      <th>numbers</th>\n",
       "      <th>Score_B.1</th>\n",
       "      <th>...</th>\n",
       "      <th>Audit_Risk</th>\n",
       "      <th>Risk</th>\n",
       "      <th>SCORE_A</th>\n",
       "      <th>SCORE_B</th>\n",
       "      <th>Marks</th>\n",
       "      <th>MONEY_Marks</th>\n",
       "      <th>District</th>\n",
       "      <th>Loss</th>\n",
       "      <th>LOSS_SCORE</th>\n",
       "      <th>History_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.00000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21.458752</td>\n",
       "      <td>2.638337</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>1.490003</td>\n",
       "      <td>13.189666</td>\n",
       "      <td>0.320640</td>\n",
       "      <td>7.767216</td>\n",
       "      <td>15.788643</td>\n",
       "      <td>5.082400</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>...</td>\n",
       "      <td>8.747667</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>3.392000</td>\n",
       "      <td>3.206400</td>\n",
       "      <td>2.28800</td>\n",
       "      <td>3.110400</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>2.070400</td>\n",
       "      <td>2.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24.583572</td>\n",
       "      <td>6.295894</td>\n",
       "      <td>0.180853</td>\n",
       "      <td>3.805486</td>\n",
       "      <td>55.547565</td>\n",
       "      <td>0.176794</td>\n",
       "      <td>33.354451</td>\n",
       "      <td>56.883767</td>\n",
       "      <td>0.291455</td>\n",
       "      <td>0.088085</td>\n",
       "      <td>...</td>\n",
       "      <td>42.943459</td>\n",
       "      <td>0.500323</td>\n",
       "      <td>1.808527</td>\n",
       "      <td>1.767937</td>\n",
       "      <td>0.88085</td>\n",
       "      <td>1.710349</td>\n",
       "      <td>1.330124</td>\n",
       "      <td>0.197315</td>\n",
       "      <td>0.402123</td>\n",
       "      <td>0.748701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.890000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55.570000</td>\n",
       "      <td>2.920000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.752000</td>\n",
       "      <td>6.720000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3.448000</td>\n",
       "      <td>11.120000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.971600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.850000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1264.630000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>758.778000</td>\n",
       "      <td>1268.910000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>961.514400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sector_score      PARA_A     Score_A      Risk_A       PARA_B  \\\n",
       "count    625.000000  625.000000  625.000000  625.000000   625.000000   \n",
       "mean      21.458752    2.638337    0.339200    1.490003    13.189666   \n",
       "std       24.583572    6.295894    0.180853    3.805486    55.547565   \n",
       "min        1.850000    0.000000    0.200000    0.000000     0.000000   \n",
       "25%        2.720000    0.000000    0.200000    0.000000     0.000000   \n",
       "50%        3.890000    0.650000    0.200000    0.130000     0.430000   \n",
       "75%       55.570000    2.920000    0.600000    1.752000     6.720000   \n",
       "max       59.850000   85.000000    0.600000   51.000000  1264.630000   \n",
       "\n",
       "          Score_B      Risk_B        TOTAL     numbers   Score_B.1  ...  \\\n",
       "count  625.000000  625.000000   625.000000  625.000000  625.000000  ...   \n",
       "mean     0.320640    7.767216    15.788643    5.082400    0.228800  ...   \n",
       "std      0.176794   33.354451    56.883767    0.291455    0.088085  ...   \n",
       "min      0.200000    0.000000     0.000000    5.000000    0.200000  ...   \n",
       "25%      0.200000    0.000000     0.210000    5.000000    0.200000  ...   \n",
       "50%      0.200000    0.086000     1.050000    5.000000    0.200000  ...   \n",
       "75%      0.600000    3.448000    11.120000    5.000000    0.200000  ...   \n",
       "max      0.600000  758.778000  1268.910000    9.000000    0.600000  ...   \n",
       "\n",
       "       Audit_Risk        Risk     SCORE_A     SCORE_B      Marks  MONEY_Marks  \\\n",
       "count  625.000000  625.000000  625.000000  625.000000  625.00000   625.000000   \n",
       "mean     8.747667    0.491200    3.392000    3.206400    2.28800     3.110400   \n",
       "std     42.943459    0.500323    1.808527    1.767937    0.88085     1.710349   \n",
       "min      0.280000    0.000000    2.000000    2.000000    2.00000     2.000000   \n",
       "25%      0.299200    0.000000    2.000000    2.000000    2.00000     2.000000   \n",
       "50%      0.444800    0.000000    2.000000    2.000000    2.00000     2.000000   \n",
       "75%      4.971600    1.000000    6.000000    6.000000    2.00000     4.000000   \n",
       "max    961.514400    1.000000    6.000000    6.000000    6.00000     6.000000   \n",
       "\n",
       "         District        Loss  LOSS_SCORE  History_score  \n",
       "count  625.000000  625.000000  625.000000     625.000000  \n",
       "mean     2.560000    0.033600    2.070400       2.204800  \n",
       "std      1.330124    0.197315    0.402123       0.748701  \n",
       "min      2.000000    0.000000    2.000000       2.000000  \n",
       "25%      2.000000    0.000000    2.000000       2.000000  \n",
       "50%      2.000000    0.000000    2.000000       2.000000  \n",
       "75%      2.000000    0.000000    2.000000       2.000000  \n",
       "max      6.000000    2.000000    6.000000       6.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_audit_risk_data.info()\n",
    "merged_audit_risk_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = merged_audit_risk_data.loc[:, ~merged_audit_risk_data.columns.isin(['Audit_Risk', 'Risk'])]\n",
    "y = merged_audit_risk_data.loc[:, merged_audit_risk_data.columns.isin(['Audit_Risk', 'Risk'])]\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier(random_state=0)\n",
    "model.fit(X, y['Risk'])\n",
    "\n",
    "%matplotlib inline\n",
    "def plot_feature_importances(model):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    n_features = X.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), X.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "\n",
    "plot_feature_importances(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PARA_A', 'Risk_A'),\n",
       " ('Score_A', 'SCORE_A'),\n",
       " ('PARA_B', 'Risk_B'),\n",
       " ('PARA_B', 'TOTAL'),\n",
       " ('Score_B', 'Score'),\n",
       " ('Score_B', 'SCORE_B'),\n",
       " ('Risk_B', 'TOTAL'),\n",
       " ('numbers', 'Score_B.1'),\n",
       " ('numbers', 'Risk_C'),\n",
       " ('numbers', 'Marks'),\n",
       " ('Score_B.1', 'Risk_C'),\n",
       " ('Score_B.1', 'Marks'),\n",
       " ('Risk_C', 'Marks'),\n",
       " ('Money_Value', 'Risk_D'),\n",
       " ('Score_MV', 'MONEY_Marks'),\n",
       " ('District_Loss', 'RiSk_E'),\n",
       " ('District_Loss', 'District'),\n",
       " ('PROB', 'Loss'),\n",
       " ('PROB', 'LOSS_SCORE'),\n",
       " ('RiSk_E', 'District'),\n",
       " ('History', 'Risk_F'),\n",
       " ('Prob', 'History_score'),\n",
       " ('Score', 'SCORE_B'),\n",
       " ('Loss', 'LOSS_SCORE')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = merged_audit_risk_data.corr().abs()\n",
    "high_corr_var = np.where(corr_matrix>0.9)\n",
    "high_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "high_corr_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_classification = ['TOTAL','Inherent_Risk', 'Prob', 'Score', 'CONTROL_RISK', 'District_Loss', 'Score_MV','District','MONEY_Marks']\n",
    "X = X.loc[:, features_for_classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval_org, X_test_org, y_trainval, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "\n",
    "# We have two output variables Audit_Risk and Risk\n",
    "# Assessing Audit_Risk is a regression problem whereas assessing Risk is a binary classification problem\n",
    "y_reg_trainval = y_trainval['Audit_Risk']\n",
    "y_reg_test = y_test['Audit_Risk']\n",
    "\n",
    "y_cls_trainval = y_trainval['Risk'].astype(np.int64)\n",
    "y_cls_test = y_test['Risk'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_trainval = scaler.fit_transform(X_trainval_org)\n",
    "X_test = scaler.fit_transform(X_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier - Soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.984\n",
      "KNeighborsClassifier 1.0\n",
      "SVC 0.984\n",
      "VotingClassifier 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_trainval, y_cls_trainval)\n",
    "knn_clf = KNeighborsClassifier(7)\n",
    "knn_clf.fit(X_trainval, y_cls_trainval)\n",
    "svm_clf = SVC(C = 10, probability = True)\n",
    "svm_clf.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf)], voting='soft')\n",
    "voting_clf.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, knn_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_trainval, y_cls_trainval)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_cls_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier - Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.984\n",
      "KNeighborsClassifier 1.0\n",
      "SVC 0.984\n",
      "VotingClassifier 0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_trainval, y_cls_trainval)\n",
    "knn_clf = KNeighborsClassifier(7)\n",
    "knn_clf.fit(X_trainval, y_cls_trainval)\n",
    "svm_clf = SVC(C = 10, probability = True)\n",
    "svm_clf.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf)], voting='hard')\n",
    "voting_clf.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, knn_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_trainval, y_cls_trainval)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_cls_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "bag_clf = BaggingClassifier(dt_clf, n_estimators=500, max_samples=100, bootstrap=True, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_trainval, y_cls_trainval)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_cls_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 1.00\n"
     ]
    }
   ],
   "source": [
    "bag_clf.fit(X_trainval, y_cls_trainval)\n",
    "print('Train score: {:.2f}'.format(bag_clf.score(X_trainval, y_cls_trainval)))\n",
    "print('Test score: {:.2f}'.format(bag_clf.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf.fit(X_trainval, y_cls_trainval)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_cls_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "bag_clf = BaggingClassifier(dt_clf, n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_trainval, y_cls_trainval)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging - knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_samples': 50, 'n_estimators': 200}\n",
      "Best Mean Train F1-score: 1.0000\n",
      "Best Mean Validation F1-score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "n_estimators_vals = [100, 200, 300, 400, 500]\n",
    "max_samples_vals = [10, 50, 70, 100, 120, 150, 170, 200]\n",
    "\n",
    "param_grid = dict(n_estimators= n_estimators_vals, max_samples=max_samples_vals)\n",
    "\n",
    "bag_knn = BaggingClassifier(knn,bootstrap = True, random_state= 0)\n",
    "\n",
    "grid_search = GridSearchCV(bag_knn, param_grid=dict(n_estimators= n_estimators_vals, max_samples=max_samples_vals), cv=kfold, return_train_score=True)\n",
    "grid_search.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best Mean Train F1-score: {:.4f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\n",
    "print(\"Best Mean Validation F1-score: {:.4f}\".format(grid_search.best_score_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasting - Decision tress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "pasting_clf = BaggingClassifier(dt_clf, n_estimators=500, max_samples=100, bootstrap=False, random_state=0)\n",
    "\n",
    "pasting_clf.fit(X_trainval, y_cls_trainval)\n",
    "y_pred = pasting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_cls_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 1.00\n"
     ]
    }
   ],
   "source": [
    "pasting_clf.fit(X_trainval, y_cls_trainval)\n",
    "print('Train score: {:.2f}'.format(pasting_clf.score(X_trainval, y_cls_trainval)))\n",
    "print('Test score: {:.2f}'.format(pasting_clf.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf.fit(X_trainval, y_cls_trainval)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_cls_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasting - knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_samples': 50, 'n_estimators': 100}\n",
      "Best Mean Train F1-score: 1.0000\n",
      "Best Mean Validation F1-score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "n_estimators_vals = [100, 200, 300, 400, 500]\n",
    "max_samples_vals = [10, 50, 70, 100, 120, 150, 170, 200]\n",
    "\n",
    "param_grid = dict(n_estimators= n_estimators_vals, max_samples=max_samples_vals)\n",
    "\n",
    "pasting_knn = BaggingClassifier(knn,bootstrap = False, random_state= 0)\n",
    "\n",
    "grid_search = GridSearchCV(pasting_knn, param_grid=dict(n_estimators= n_estimators_vals, max_samples=max_samples_vals), cv=kfold, return_train_score=True)\n",
    "grid_search.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best Mean Train F1-score: {:.4f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\n",
    "print(\"Best Mean Validation F1-score: {:.4f}\".format(grid_search.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Boosting - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_trainval_org, X_test_org, y_trainval, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "\n",
    "y_cls_trainval = y_trainval['Risk'].astype(np.int64)\n",
    "y_cls_test = y_test['Risk'].astype(np.int64)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_trainval, y_cls_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_cls_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 1.00\n"
     ]
    }
   ],
   "source": [
    "ada_clf.fit(X_trainval, y_cls_trainval)\n",
    "print('Train score: {:.2f}'.format(ada_clf.score(X_trainval, y_cls_trainval)))\n",
    "print('Test score: {:.2f}'.format(ada_clf.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Boosting - SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "          learning_rate=1, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "svc = SVC(probability = True, kernel = 'linear')\n",
    "\n",
    "ada_svc = AdaBoostClassifier(n_estimators= 50, base_estimator=svc, learning_rate=1)\n",
    "ada_svc.fit(X_trainval,y_cls_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ada_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_cls_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Train score: {:.2f}'.format(ada_svc.score(X_trainval, y_cls_trainval)))\n",
    "print('Test score: {:.2f}'.format(ada_svc.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_trainval_org, X_test_org, y_trainval, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_trainval, y_cls_trainval)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 1.000\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_trainval, y_cls_trainval)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 1.000\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_trainval, y_cls_trainval)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "500/500 [==============================] - 0s 960us/sample - loss: 0.6754 - acc: 0.8100\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 0s 29us/sample - loss: 0.6716 - acc: 0.9140\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 0s 27us/sample - loss: 0.6675 - acc: 0.9460\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 0s 21us/sample - loss: 0.6631 - acc: 0.9560\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.6576 - acc: 0.9740\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 0s 23us/sample - loss: 0.6513 - acc: 0.9960\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 0s 21us/sample - loss: 0.6446 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 0s 25us/sample - loss: 0.6376 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.6308 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 0s 32us/sample - loss: 0.6237 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 0s 25us/sample - loss: 0.6169 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.6097 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 0s 21us/sample - loss: 0.6022 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.5947 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 0s 31us/sample - loss: 0.5872 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 0s 20us/sample - loss: 0.5798 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 0s 24us/sample - loss: 0.5723 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 0s 28us/sample - loss: 0.5649 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 0s 23us/sample - loss: 0.5574 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 0s 22us/sample - loss: 0.5499 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ce4111d30>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: build the model\n",
    "model1 = Sequential()\n",
    "#input leyer\n",
    "model1.add(Dense(10, input_dim = 9, activation = 'relu'))\n",
    "#hidden layer\n",
    "model1.add(Dense(5, activation = 'relu'))\n",
    "#output layer\n",
    "model1.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# step 2: build the computational graph - compile\n",
    "model1.compile(loss = 'binary_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])\n",
    "\n",
    "# step 3: train the model\n",
    "model1.fit(X_trainval, y_cls_trainval, epochs = 20, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 231us/sample - loss: 0.5454 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5454196567535401, 1.0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_trainval, y_cls_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 48us/sample - loss: 0.5272 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5271959962844849, 1.0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test, y_cls_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model1.predict(X_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.where(y_pred >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_cls_trainval, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95,random_state = 0)\n",
    "\n",
    "pca.fit(X)\n",
    "X_red = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_trainval, X_test, y_train, y_test = train_test_split(X_red,y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_cls_trainval = y_train['Risk'].astype(np.int64)\n",
    "y_cls_test = y_test['Risk'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors Classification with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': range(1, 20)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': range(1, 20)}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=kfold, return_train_score=True, scoring='f1')\n",
    "grid_search.fit(X_trainval, y_cls_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 1}\n",
      "Best Mean Train F1-score: 1.0000\n",
      "Best Mean Validation F1-score: 0.9961\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 1}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996104</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_neighbors': 2}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993244</td>\n",
       "      <td>0.005567</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997333</td>\n",
       "      <td>0.997375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998942</td>\n",
       "      <td>0.001296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_neighbors': 3}</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993584</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998428</td>\n",
       "      <td>0.002088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_neighbors': 4}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995613</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>0.997389</td>\n",
       "      <td>0.997275</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998402</td>\n",
       "      <td>0.001305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.008633</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_neighbors': 5}</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991327</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>5</td>\n",
       "      <td>0.991870</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.991870</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.992544</td>\n",
       "      <td>0.002522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.007872</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_neighbors': 6}</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984686</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>8</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.994709</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.991870</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.992521</td>\n",
       "      <td>0.003082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.008846</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>7</td>\n",
       "      <td>{'n_neighbors': 7}</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982479</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>11</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.991826</td>\n",
       "      <td>0.989840</td>\n",
       "      <td>0.002026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>8</td>\n",
       "      <td>{'n_neighbors': 8}</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984686</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>8</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.989189</td>\n",
       "      <td>0.991826</td>\n",
       "      <td>0.989840</td>\n",
       "      <td>0.001009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>9</td>\n",
       "      <td>{'n_neighbors': 9}</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982325</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>14</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.989301</td>\n",
       "      <td>0.001768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_neighbors': 10}</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984806</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>7</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.991826</td>\n",
       "      <td>0.989222</td>\n",
       "      <td>0.003073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>11</td>\n",
       "      <td>{'n_neighbors': 11}</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987165</td>\n",
       "      <td>0.003864</td>\n",
       "      <td>6</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.994565</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.991962</td>\n",
       "      <td>0.003383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>12</td>\n",
       "      <td>{'n_neighbors': 12}</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973306</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>19</td>\n",
       "      <td>0.986376</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.980716</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.985946</td>\n",
       "      <td>0.003335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>13</td>\n",
       "      <td>{'n_neighbors': 13}</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980084</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>16</td>\n",
       "      <td>0.991870</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.987605</td>\n",
       "      <td>0.003326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>14</td>\n",
       "      <td>{'n_neighbors': 14}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982304</td>\n",
       "      <td>0.018225</td>\n",
       "      <td>15</td>\n",
       "      <td>0.997275</td>\n",
       "      <td>0.983784</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>0.980716</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.989139</td>\n",
       "      <td>0.006289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>15</td>\n",
       "      <td>{'n_neighbors': 15}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984533</td>\n",
       "      <td>0.015403</td>\n",
       "      <td>10</td>\n",
       "      <td>0.997275</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.994792</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.989717</td>\n",
       "      <td>0.005583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>16</td>\n",
       "      <td>{'n_neighbors': 16}</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982463</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>12</td>\n",
       "      <td>0.980609</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.997389</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.988529</td>\n",
       "      <td>0.007270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.009496</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>17</td>\n",
       "      <td>{'n_neighbors': 17}</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982463</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>12</td>\n",
       "      <td>0.980609</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.997389</td>\n",
       "      <td>0.983516</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.988529</td>\n",
       "      <td>0.007270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>18</td>\n",
       "      <td>{'n_neighbors': 18}</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979983</td>\n",
       "      <td>0.011302</td>\n",
       "      <td>17</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.981432</td>\n",
       "      <td>0.980716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984759</td>\n",
       "      <td>0.007864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>19</td>\n",
       "      <td>{'n_neighbors': 19}</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979983</td>\n",
       "      <td>0.011302</td>\n",
       "      <td>17</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.986807</td>\n",
       "      <td>0.980716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985834</td>\n",
       "      <td>0.007702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.003271      0.001859         0.012117        0.005574   \n",
       "1        0.002233      0.001667         0.007375        0.000980   \n",
       "2        0.001393      0.000807         0.004805        0.002200   \n",
       "3        0.001094      0.000672         0.006608        0.001763   \n",
       "4        0.002630      0.001353         0.008633        0.002477   \n",
       "5        0.002911      0.001345         0.007872        0.002060   \n",
       "6        0.002596      0.001354         0.008846        0.001048   \n",
       "7        0.002246      0.000903         0.009615        0.001562   \n",
       "8        0.001796      0.000400         0.006822        0.001445   \n",
       "9        0.001938      0.000930         0.008916        0.002369   \n",
       "10       0.000896      0.000791         0.007028        0.001824   \n",
       "11       0.001382      0.001325         0.007937        0.001036   \n",
       "12       0.001728      0.001323         0.009847        0.001667   \n",
       "13       0.001302      0.001315         0.008696        0.001960   \n",
       "14       0.001112      0.000671         0.006491        0.000905   \n",
       "15       0.002056      0.000520         0.011933        0.002751   \n",
       "16       0.002038      0.001515         0.009496        0.001343   \n",
       "17       0.001187      0.000679         0.008637        0.001383   \n",
       "18       0.001090      0.000740         0.007257        0.000404   \n",
       "\n",
       "   param_n_neighbors               params  split0_test_score  \\\n",
       "0                  1   {'n_neighbors': 1}           1.000000   \n",
       "1                  2   {'n_neighbors': 2}           1.000000   \n",
       "2                  3   {'n_neighbors': 3}           0.989899   \n",
       "3                  4   {'n_neighbors': 4}           1.000000   \n",
       "4                  5   {'n_neighbors': 5}           0.989899   \n",
       "5                  6   {'n_neighbors': 6}           0.979592   \n",
       "6                  7   {'n_neighbors': 7}           0.979592   \n",
       "7                  8   {'n_neighbors': 8}           0.979592   \n",
       "8                  9   {'n_neighbors': 9}           0.979592   \n",
       "9                 10  {'n_neighbors': 10}           0.979592   \n",
       "10                11  {'n_neighbors': 11}           0.979592   \n",
       "11                12  {'n_neighbors': 12}           0.979592   \n",
       "12                13  {'n_neighbors': 13}           0.989899   \n",
       "13                14  {'n_neighbors': 14}           1.000000   \n",
       "14                15  {'n_neighbors': 15}           1.000000   \n",
       "15                16  {'n_neighbors': 16}           0.989691   \n",
       "16                17  {'n_neighbors': 17}           0.989691   \n",
       "17                18  {'n_neighbors': 18}           0.989691   \n",
       "18                19  {'n_neighbors': 19}           0.989691   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0            1.000000           1.000000           1.000000  ...   \n",
       "1            0.988506           0.987654           1.000000  ...   \n",
       "2            1.000000           0.987952           1.000000  ...   \n",
       "3            1.000000           0.987952           1.000000  ...   \n",
       "4            0.988764           0.987952           1.000000  ...   \n",
       "5            0.988764           0.975610           0.989474  ...   \n",
       "6            0.977778           0.975610           0.989474  ...   \n",
       "7            0.988764           0.975610           0.989474  ...   \n",
       "8            0.988764           0.963855           0.989474  ...   \n",
       "9            1.000000           0.975610           0.978723  ...   \n",
       "10           0.988764           0.987952           0.989474  ...   \n",
       "11           0.964706           0.975610           0.956522  ...   \n",
       "12           0.964706           0.987952           0.967742  ...   \n",
       "13           0.964706           1.000000           0.956522  ...   \n",
       "14           0.964706           1.000000           0.967742  ...   \n",
       "15           0.964706           1.000000           0.967742  ...   \n",
       "16           0.964706           1.000000           0.967742  ...   \n",
       "17           0.964706           0.987654           0.967742  ...   \n",
       "18           0.964706           0.987654           0.967742  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0          0.996104        0.007824                1            1.000000   \n",
       "1          0.993244        0.005567                4            1.000000   \n",
       "2          0.993584        0.005288                3            1.000000   \n",
       "3          0.995613        0.005421                2            1.000000   \n",
       "4          0.991327        0.004389                5            0.991870   \n",
       "5          0.984686        0.005956                8            0.989130   \n",
       "6          0.982479        0.006071               11            0.989130   \n",
       "7          0.984686        0.005956                8            0.989130   \n",
       "8          0.982325        0.010025               14            0.989130   \n",
       "9          0.984806        0.009041                7            0.989130   \n",
       "10         0.987165        0.003864                6            0.989130   \n",
       "11         0.973306        0.011657               19            0.986376   \n",
       "12         0.980084        0.011379               16            0.991870   \n",
       "13         0.982304        0.018225               15            0.997275   \n",
       "14         0.984533        0.015403               10            0.997275   \n",
       "15         0.982463        0.013793               12            0.980609   \n",
       "16         0.982463        0.013793               12            0.980609   \n",
       "17         0.979983        0.011302               17            0.977778   \n",
       "18         0.979983        0.011302               17            0.977778   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0             1.000000            1.000000            1.000000   \n",
       "1             0.997333            0.997375            1.000000   \n",
       "2             0.997347            0.994792            1.000000   \n",
       "3             0.997347            0.997389            0.997275   \n",
       "4             0.992084            0.989637            0.991870   \n",
       "5             0.994709            0.989637            0.991870   \n",
       "6             0.992084            0.989637            0.986523   \n",
       "7             0.989418            0.989637            0.989189   \n",
       "8             0.992084            0.989637            0.986523   \n",
       "9             0.992000            0.989637            0.983516   \n",
       "10            0.997347            0.989637            0.994565   \n",
       "11            0.983871            0.989637            0.980716   \n",
       "12            0.983871            0.989637            0.983516   \n",
       "13            0.983784            0.994792            0.980716   \n",
       "14            0.983871            0.994792            0.983516   \n",
       "15            0.983871            0.997389            0.983516   \n",
       "16            0.983871            0.997389            0.983516   \n",
       "17            0.983871            0.981432            0.980716   \n",
       "18            0.983871            0.986807            0.980716   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0             1.000000          1.000000         0.000000  \n",
       "1             1.000000          0.998942         0.001296  \n",
       "2             1.000000          0.998428         0.002088  \n",
       "3             1.000000          0.998402         0.001305  \n",
       "4             0.997260          0.992544         0.002522  \n",
       "5             0.997260          0.992521         0.003082  \n",
       "6             0.991826          0.989840         0.002026  \n",
       "7             0.991826          0.989840         0.001009  \n",
       "8             0.989130          0.989301         0.001768  \n",
       "9             0.991826          0.989222         0.003073  \n",
       "10            0.989130          0.991962         0.003383  \n",
       "11            0.989130          0.985946         0.003335  \n",
       "12            0.989130          0.987605         0.003326  \n",
       "13            0.989130          0.989139         0.006289  \n",
       "14            0.989130          0.989717         0.005583  \n",
       "15            0.997260          0.988529         0.007270  \n",
       "16            0.997260          0.988529         0.007270  \n",
       "17            1.000000          0.984759         0.007864  \n",
       "18            1.000000          0.985834         0.007702  \n",
       "\n",
       "[19 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best Mean Train F1-score: {:.4f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\n",
    "print(\"Best Mean Validation F1-score: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        82\n",
      "           1       0.95      1.00      0.97        75\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       157\n",
      "   macro avg       0.97      0.98      0.97       157\n",
      "weighted avg       0.98      0.97      0.97       157\n",
      "\n",
      "Accuracy: 0.9745\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(1)\n",
    "knn.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_cls_test, knn.predict(X_test)))\n",
    "print('Accuracy: {:.4f}'.format(knn.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn classification without PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Mean Train F1-score is  1.0000\n",
    "\n",
    "The Mean Validation F1-score is 0.9956\n",
    "\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        60\n",
    "           1       1.00      1.00      1.00        65\n",
    "\n",
    "Accuracy: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "             'penalty': ['l1', 'l2']}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=kfold, return_train_score=True, scoring='f1')\n",
    "grid_search.fit(X_trainval, y_cls_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1000, 'penalty': 'l2'}\n",
      "Best Mean Train F1-score: 0.9946\n",
      "Best Mean Validation F1-score: 0.9928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l1'}</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635594</td>\n",
       "      <td>0.055817</td>\n",
       "      <td>13</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.652330</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>0.652015</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.639038</td>\n",
       "      <td>0.015422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l2'}</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638088</td>\n",
       "      <td>0.060416</td>\n",
       "      <td>12</td>\n",
       "      <td>0.649446</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.640569</td>\n",
       "      <td>0.661818</td>\n",
       "      <td>0.615970</td>\n",
       "      <td>0.644989</td>\n",
       "      <td>0.016202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1'}</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635594</td>\n",
       "      <td>0.055817</td>\n",
       "      <td>13</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.652330</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>0.652015</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.639038</td>\n",
       "      <td>0.015422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l2'}</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666902</td>\n",
       "      <td>0.045358</td>\n",
       "      <td>11</td>\n",
       "      <td>0.669091</td>\n",
       "      <td>0.685315</td>\n",
       "      <td>0.678201</td>\n",
       "      <td>0.681004</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.673092</td>\n",
       "      <td>0.011876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864354</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>9</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>0.877612</td>\n",
       "      <td>0.869822</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.867497</td>\n",
       "      <td>0.005670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2'}</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798626</td>\n",
       "      <td>0.027591</td>\n",
       "      <td>10</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.798680</td>\n",
       "      <td>0.803949</td>\n",
       "      <td>0.007782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009113</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>4</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.928775</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.925854</td>\n",
       "      <td>0.005577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887928</td>\n",
       "      <td>0.026117</td>\n",
       "      <td>8</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.900585</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>0.894895</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.892469</td>\n",
       "      <td>0.006954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.188822</td>\n",
       "      <td>0.028159</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923690</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>3</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.928775</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.927670</td>\n",
       "      <td>0.006985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>4</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.929972</td>\n",
       "      <td>0.927114</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.925820</td>\n",
       "      <td>0.005365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.170401</td>\n",
       "      <td>0.037744</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 100, 'penalty': 'l1'}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>4</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929537</td>\n",
       "      <td>0.006782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 100, 'penalty': 'l2'}</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968957</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>2</td>\n",
       "      <td>0.963173</td>\n",
       "      <td>0.975477</td>\n",
       "      <td>0.978610</td>\n",
       "      <td>0.963380</td>\n",
       "      <td>0.977654</td>\n",
       "      <td>0.971659</td>\n",
       "      <td>0.006919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.166174</td>\n",
       "      <td>0.034046</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1000, 'penalty': 'l1'}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>4</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.928775</td>\n",
       "      <td>0.929972</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.927072</td>\n",
       "      <td>0.006625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 1000, 'penalty': 'l2'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992844</td>\n",
       "      <td>0.009677</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991826</td>\n",
       "      <td>0.994681</td>\n",
       "      <td>0.997389</td>\n",
       "      <td>0.994565</td>\n",
       "      <td>0.994536</td>\n",
       "      <td>0.994599</td>\n",
       "      <td>0.001760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.002094      0.003709         0.001455        0.001790   0.001   \n",
       "1        0.003258      0.001650         0.000993        0.001534   0.001   \n",
       "2        0.003460      0.001371         0.000400        0.000490    0.01   \n",
       "3        0.002073      0.001591         0.000990        0.001532    0.01   \n",
       "4        0.002189      0.002037         0.002041        0.001706     0.1   \n",
       "5        0.000989      0.001529         0.001345        0.001689     0.1   \n",
       "6        0.009113      0.003555         0.001775        0.001794       1   \n",
       "7        0.003378      0.001086         0.000199        0.000398       1   \n",
       "8        0.188822      0.028159         0.002271        0.001145      10   \n",
       "9        0.004881      0.001827         0.001767        0.000612      10   \n",
       "10       0.170401      0.037744         0.001532        0.001477     100   \n",
       "11       0.002174      0.001587         0.001482        0.001472     100   \n",
       "12       0.166174      0.034046         0.001885        0.000220    1000   \n",
       "13       0.002419      0.001224         0.001254        0.000735    1000   \n",
       "\n",
       "   param_penalty                         params  split0_test_score  \\\n",
       "0             l1  {'C': 0.001, 'penalty': 'l1'}           0.619718   \n",
       "1             l2  {'C': 0.001, 'penalty': 'l2'}           0.638889   \n",
       "2             l1   {'C': 0.01, 'penalty': 'l1'}           0.619718   \n",
       "3             l2   {'C': 0.01, 'penalty': 'l2'}           0.675676   \n",
       "4             l1    {'C': 0.1, 'penalty': 'l1'}           0.860465   \n",
       "5             l2    {'C': 0.1, 'penalty': 'l2'}           0.819277   \n",
       "6             l1      {'C': 1, 'penalty': 'l1'}           0.923077   \n",
       "7             l2      {'C': 1, 'penalty': 'l2'}           0.886364   \n",
       "8             l1     {'C': 10, 'penalty': 'l1'}           0.923077   \n",
       "9             l2     {'C': 10, 'penalty': 'l2'}           0.923077   \n",
       "10            l1    {'C': 100, 'penalty': 'l1'}           0.923077   \n",
       "11            l2    {'C': 100, 'penalty': 'l2'}           0.957447   \n",
       "12            l1   {'C': 1000, 'penalty': 'l1'}           0.923077   \n",
       "13            l2   {'C': 1000, 'penalty': 'l2'}           1.000000   \n",
       "\n",
       "    split1_test_score  split2_test_score  ...  mean_test_score  \\\n",
       "0            0.580645           0.655738  ...         0.635594   \n",
       "1            0.580645           0.633333  ...         0.638088   \n",
       "2            0.580645           0.655738  ...         0.635594   \n",
       "3            0.625000           0.655738  ...         0.666902   \n",
       "4            0.826667           0.876712  ...         0.864354   \n",
       "5            0.794521           0.757576  ...         0.798626   \n",
       "6            0.886076           0.906667  ...         0.920800   \n",
       "7            0.857143           0.876712  ...         0.887928   \n",
       "8            0.886076           0.921053  ...         0.923690   \n",
       "9            0.886076           0.906667  ...         0.920800   \n",
       "10           0.886076           0.906667  ...         0.920800   \n",
       "11           0.976744           0.975610  ...         0.968957   \n",
       "12           0.886076           0.906667  ...         0.920800   \n",
       "13           0.988764           0.975610  ...         0.992844   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         0.055817               13            0.644444            0.652330   \n",
       "1         0.060416               12            0.649446            0.657143   \n",
       "2         0.055817               13            0.644444            0.652330   \n",
       "3         0.045358               11            0.669091            0.685315   \n",
       "4         0.024447                9            0.863354            0.877612   \n",
       "5         0.027591               10            0.792079            0.810127   \n",
       "6         0.031582                4            0.923529            0.928775   \n",
       "7         0.026117                8            0.890909            0.900585   \n",
       "8         0.030806                3            0.923529            0.928775   \n",
       "9         0.031582                4            0.923529            0.931818   \n",
       "10        0.031582                4            0.929825            0.931818   \n",
       "11        0.015759                2            0.963173            0.975477   \n",
       "12        0.031582                4            0.923529            0.928775   \n",
       "13        0.009677                1            0.991826            0.994681   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0             0.635714            0.652015            0.610687   \n",
       "1             0.640569            0.661818            0.615970   \n",
       "2             0.635714            0.652015            0.610687   \n",
       "3             0.678201            0.681004            0.651852   \n",
       "4             0.869822            0.864198            0.862500   \n",
       "5             0.813665            0.805195            0.798680   \n",
       "6             0.926966            0.933333            0.916667   \n",
       "7             0.895954            0.894895            0.880000   \n",
       "8             0.932961            0.936416            0.916667   \n",
       "9             0.929972            0.927114            0.916667   \n",
       "10            0.932961            0.936416            0.916667   \n",
       "11            0.978610            0.963380            0.977654   \n",
       "12            0.929972            0.936416            0.916667   \n",
       "13            0.997389            0.994565            0.994536   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "0           0.639038         0.015422  \n",
       "1           0.644989         0.016202  \n",
       "2           0.639038         0.015422  \n",
       "3           0.673092         0.011876  \n",
       "4           0.867497         0.005670  \n",
       "5           0.803949         0.007782  \n",
       "6           0.925854         0.005577  \n",
       "7           0.892469         0.006954  \n",
       "8           0.927670         0.006985  \n",
       "9           0.925820         0.005365  \n",
       "10          0.929537         0.006782  \n",
       "11          0.971659         0.006919  \n",
       "12          0.927072         0.006625  \n",
       "13          0.994599         0.001760  \n",
       "\n",
       "[14 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best Mean Train F1-score: {:.4f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\n",
    "print(\"Best Mean Validation F1-score: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        82\n",
      "           1       1.00      0.92      0.96        75\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       157\n",
      "   macro avg       0.97      0.96      0.96       157\n",
      "weighted avg       0.96      0.96      0.96       157\n",
      "\n",
      "Accuracy: 0.9618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression(penalty = 'l1', C = 10)\n",
    "lreg.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_cls_test, lreg.predict(X_test)))\n",
    "print('Accuracy: {:.4f}'.format(lreg.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression without PCA\n",
    "\n",
    "\n",
    "The Mean Train F1-score is 1.0000\n",
    "\n",
    "The Mean Validation F1-score is 1.0000\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        60\n",
    "           1       1.00      1.00      1.00        65\n",
    "\n",
    "Accuracy: 1.0000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_search = GridSearchCV(LinearSVC(), param_grid, cv=kfold, return_train_score=True, scoring='f1')\n",
    "grid_search.fit(X_trainval, y_cls_trainval)\n",
    "train_score_array = []\n",
    "test_score_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 100}\n",
      "Best Mean Train F1-score: 0.9232\n",
      "Best Mean Validation F1-score: 0.9208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 0.001}</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657993</td>\n",
       "      <td>0.055854</td>\n",
       "      <td>7</td>\n",
       "      <td>0.669091</td>\n",
       "      <td>0.680702</td>\n",
       "      <td>0.678201</td>\n",
       "      <td>0.681004</td>\n",
       "      <td>0.636704</td>\n",
       "      <td>0.669140</td>\n",
       "      <td>0.016785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005505</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778758</td>\n",
       "      <td>0.035733</td>\n",
       "      <td>5</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.798722</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.792222</td>\n",
       "      <td>0.009239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024056</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885357</td>\n",
       "      <td>0.026767</td>\n",
       "      <td>4</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.897361</td>\n",
       "      <td>0.892754</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.890518</td>\n",
       "      <td>0.005722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019224</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913350</td>\n",
       "      <td>0.032371</td>\n",
       "      <td>3</td>\n",
       "      <td>0.913947</td>\n",
       "      <td>0.922636</td>\n",
       "      <td>0.914773</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.896970</td>\n",
       "      <td>0.913194</td>\n",
       "      <td>0.008663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018540</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918309</td>\n",
       "      <td>0.032876</td>\n",
       "      <td>2</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.928775</td>\n",
       "      <td>0.923944</td>\n",
       "      <td>0.927114</td>\n",
       "      <td>0.896970</td>\n",
       "      <td>0.919431</td>\n",
       "      <td>0.011594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>100</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>0.031582</td>\n",
       "      <td>1</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.928775</td>\n",
       "      <td>0.923944</td>\n",
       "      <td>0.939481</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.923234</td>\n",
       "      <td>0.011734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.021380</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'C': 1000}</td>\n",
       "      <td>0.685315</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771990</td>\n",
       "      <td>0.102308</td>\n",
       "      <td>6</td>\n",
       "      <td>0.657092</td>\n",
       "      <td>0.928775</td>\n",
       "      <td>0.923944</td>\n",
       "      <td>0.658318</td>\n",
       "      <td>0.653501</td>\n",
       "      <td>0.764326</td>\n",
       "      <td>0.132318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.004648      0.002267         0.001593        0.000522   0.001   \n",
       "1       0.005505      0.002537         0.002058        0.002190    0.01   \n",
       "2       0.024056      0.004029         0.002574        0.001616     0.1   \n",
       "3       0.019224      0.001467         0.001334        0.000908       1   \n",
       "4       0.018540      0.001102         0.001623        0.001012      10   \n",
       "5       0.019583      0.001355         0.002239        0.001345     100   \n",
       "6       0.021380      0.001488         0.002582        0.000700    1000   \n",
       "\n",
       "         params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'C': 0.001}           0.675676           0.580645           0.655738   \n",
       "1   {'C': 0.01}           0.804878           0.777778           0.718750   \n",
       "2    {'C': 0.1}           0.873563           0.857143           0.876712   \n",
       "3      {'C': 1}           0.923077           0.871795           0.906667   \n",
       "4     {'C': 10}           0.923077           0.886076           0.906667   \n",
       "5    {'C': 100}           0.923077           0.886076           0.906667   \n",
       "6   {'C': 1000}           0.685315           0.886076           0.906667   \n",
       "\n",
       "   split3_test_score  ...  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0           0.628571  ...         0.657993        0.055854                7   \n",
       "1           0.769231  ...         0.778758        0.035733                5   \n",
       "2           0.883721  ...         0.885357        0.026767                4   \n",
       "3           0.896552  ...         0.913350        0.032371                3   \n",
       "4           0.896552  ...         0.918309        0.032876                2   \n",
       "5           0.909091  ...         0.920800        0.031582                1   \n",
       "6           0.680851  ...         0.771990        0.102308                6   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0            0.669091            0.680702            0.678201   \n",
       "1            0.780000            0.798722            0.806250   \n",
       "2            0.890909            0.897361            0.892754   \n",
       "3            0.913947            0.922636            0.914773   \n",
       "4            0.920354            0.928775            0.923944   \n",
       "5            0.920354            0.928775            0.923944   \n",
       "6            0.657092            0.928775            0.923944   \n",
       "\n",
       "   split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.681004            0.636704          0.669140         0.016785  \n",
       "1            0.789474            0.786667          0.792222         0.009239  \n",
       "2            0.891566            0.880000          0.890518         0.005722  \n",
       "3            0.917647            0.896970          0.913194         0.008663  \n",
       "4            0.927114            0.896970          0.919431         0.011594  \n",
       "5            0.939481            0.903614          0.923234         0.011734  \n",
       "6            0.658318            0.653501          0.764326         0.132318  \n",
       "\n",
       "[7 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best Mean Train F1-score: {:.4f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\n",
    "print(\"Best Mean Validation F1-score: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        82\n",
      "           1       1.00      0.92      0.96        75\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       157\n",
      "   macro avg       0.97      0.96      0.96       157\n",
      "weighted avg       0.96      0.96      0.96       157\n",
      "\n",
      "Accuracy: 0.9618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(C = 1)\n",
    "lsvc.fit(X_trainval,y_cls_trainval)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_cls_test, lsvc.predict(X_test)))\n",
    "print('Accuracy: {:.4f}'.format(lsvc.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine without PCA\n",
    "\n",
    "The Mean Train F1-score is 1.0000\n",
    "\n",
    "The Mean Validation F1-score is 0.9956\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        60\n",
    "           1       1.00      1.00      1.00        65\n",
    "\n",
    "Accuracy: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelized Support Vector Machine with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\sanjana\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'rbf', 'poly']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                 'gamma': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                 'kernel': ['linear', 'rbf', 'poly']}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=kfold, return_train_score=True, scoring='f1')\n",
    "grid_search.fit(X_trainval, y_cls_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 100, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "Best Mean Train F1-score: 1.0000\n",
      "Best Mean Validation F1-score: 0.9980\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best Mean Train F1-score: {:.4f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\n",
    "print(\"Best Mean Validation F1-score: {:.4f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.001, 'kernel': 'linear'}</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890442</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>124</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.894408</td>\n",
       "      <td>0.008302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006365</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250082</td>\n",
       "      <td>0.305349</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.676106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269029</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346326</td>\n",
       "      <td>0.050141</td>\n",
       "      <td>137</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.336283</td>\n",
       "      <td>0.360515</td>\n",
       "      <td>0.342342</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.350133</td>\n",
       "      <td>0.011280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.01, 'kernel': 'linear'}</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890442</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>124</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.894408</td>\n",
       "      <td>0.008302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007774</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250082</td>\n",
       "      <td>0.305349</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.676106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269029</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908812</td>\n",
       "      <td>0.028185</td>\n",
       "      <td>119</td>\n",
       "      <td>0.907463</td>\n",
       "      <td>0.916427</td>\n",
       "      <td>0.917847</td>\n",
       "      <td>0.914454</td>\n",
       "      <td>0.896970</td>\n",
       "      <td>0.910632</td>\n",
       "      <td>0.007706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.1, 'kernel': 'linear'}</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890442</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>124</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.894408</td>\n",
       "      <td>0.008302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007627</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250082</td>\n",
       "      <td>0.305349</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.676106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269029</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.001, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.009561</td>\n",
       "      <td>84</td>\n",
       "      <td>0.989189</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.987080</td>\n",
       "      <td>0.994595</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.990416</td>\n",
       "      <td>0.002628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890442</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>124</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.894408</td>\n",
       "      <td>0.008302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.015593</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1, 'kernel': 'rbf'}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250082</td>\n",
       "      <td>0.305349</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.676106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269029</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003225</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.001, 'gamma': 10, 'kernel': 'linear'}</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890442</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>124</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.894408</td>\n",
       "      <td>0.008302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.016480</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.006443</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.001, 'gamma': 10, 'kernel': 'rbf'}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250082</td>\n",
       "      <td>0.305349</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.676106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269029</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.001, 'gamma': 10, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.001, 'gamma': 100, 'kernel': 'linear'}</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890442</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>124</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.894408</td>\n",
       "      <td>0.008302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.013078</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.001, 'gamma': 100, 'kernel': 'rbf'}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250082</td>\n",
       "      <td>0.305349</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.676106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269029</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.001, 'gamma': 100, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1000, 'kernel': 'linear'}</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890442</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>124</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.898204</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.894408</td>\n",
       "      <td>0.008302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.018918</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1000, 'kernel': 'rbf'}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250082</td>\n",
       "      <td>0.305349</td>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.676106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269029</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1000</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.001, 'gamma': 1000, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.001, 'kernel': 'linear'}</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923151</td>\n",
       "      <td>0.032096</td>\n",
       "      <td>108</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.929972</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.913433</td>\n",
       "      <td>0.926417</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.009298</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652529</td>\n",
       "      <td>0.152723</td>\n",
       "      <td>135</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.879765</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>0.495868</td>\n",
       "      <td>0.661404</td>\n",
       "      <td>0.153526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.004971</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474383</td>\n",
       "      <td>0.042587</td>\n",
       "      <td>136</td>\n",
       "      <td>0.487603</td>\n",
       "      <td>0.477733</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.485597</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.476112</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.01, 'kernel': 'linear'}</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923151</td>\n",
       "      <td>0.032096</td>\n",
       "      <td>108</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.929972</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.913433</td>\n",
       "      <td>0.926417</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.008414</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.875778</td>\n",
       "      <td>0.014271</td>\n",
       "      <td>132</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.903790</td>\n",
       "      <td>0.899135</td>\n",
       "      <td>0.871166</td>\n",
       "      <td>0.858934</td>\n",
       "      <td>0.880679</td>\n",
       "      <td>0.017575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929599</td>\n",
       "      <td>0.021787</td>\n",
       "      <td>105</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.934844</td>\n",
       "      <td>0.936288</td>\n",
       "      <td>0.933718</td>\n",
       "      <td>0.932551</td>\n",
       "      <td>0.933445</td>\n",
       "      <td>0.002192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.004236</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.1, 'kernel': 'linear'}</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923151</td>\n",
       "      <td>0.032096</td>\n",
       "      <td>108</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.929972</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.913433</td>\n",
       "      <td>0.926417</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929475</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>107</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.930748</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.913433</td>\n",
       "      <td>0.926029</td>\n",
       "      <td>0.006971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.006798</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992844</td>\n",
       "      <td>0.009677</td>\n",
       "      <td>63</td>\n",
       "      <td>0.994565</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997290</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.997293</td>\n",
       "      <td>0.001719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 100, 'gamma': 10, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.014534</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 100, 'gamma': 10, 'kernel': 'rbf'}</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994075</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 100, 'gamma': 10, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 100, 'gamma': 100, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.020933</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 100, 'gamma': 100, 'kernel': 'rbf'}</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974540</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>89</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 100, 'gamma': 100, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 100, 'gamma': 1000, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.020395</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 100, 'gamma': 1000, 'kernel': 'rbf'}</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915487</td>\n",
       "      <td>0.033797</td>\n",
       "      <td>115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 100, 'gamma': 1000, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.002852</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991149</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>69</td>\n",
       "      <td>0.991870</td>\n",
       "      <td>0.994709</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>0.994595</td>\n",
       "      <td>0.991826</td>\n",
       "      <td>0.992527</td>\n",
       "      <td>0.001914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.004739</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.009561</td>\n",
       "      <td>84</td>\n",
       "      <td>0.989189</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.987080</td>\n",
       "      <td>0.994595</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.990416</td>\n",
       "      <td>0.002628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.01, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997580</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>16</td>\n",
       "      <td>0.997275</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997290</td>\n",
       "      <td>0.997260</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>0.001083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.1, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1000, 'gamma': 1, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.005112</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 1000, 'gamma': 1, 'kernel': 'rbf'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991367</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>66</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 1000, 'gamma': 1, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1000, 'gamma': 10, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.013809</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 1000, 'gamma': 10, 'kernel': 'rbf'}</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994075</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>60</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 1000, 'gamma': 10, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1000, 'gamma': 100, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.014803</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 1000, 'gamma': 100, 'kernel': 'rbf'}</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.988764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974540</td>\n",
       "      <td>0.020368</td>\n",
       "      <td>89</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 1000, 'gamma': 100, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'C': 1000, 'gamma': 1000, 'kernel': 'linear'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.010885</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>rbf</td>\n",
       "      <td>{'C': 1000, 'gamma': 1000, 'kernel': 'rbf'}</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915487</td>\n",
       "      <td>0.033797</td>\n",
       "      <td>115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'C': 1000, 'gamma': 1000, 'kernel': 'poly'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995553</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0         0.001717      0.002745         0.001774        0.001449   0.001   \n",
       "1         0.006365      0.000766         0.003298        0.003112   0.001   \n",
       "2         0.004185      0.001183         0.001873        0.000793   0.001   \n",
       "3         0.001956      0.000719         0.001320        0.000887   0.001   \n",
       "4         0.007774      0.001298         0.002457        0.001336   0.001   \n",
       "5         0.001986      0.001254         0.000800        0.001167   0.001   \n",
       "6         0.001425      0.000821         0.000598        0.000797   0.001   \n",
       "7         0.007627      0.001043         0.002602        0.001340   0.001   \n",
       "8         0.002552      0.001499         0.001175        0.000739   0.001   \n",
       "9         0.003922      0.001175         0.001574        0.000637   0.001   \n",
       "10        0.015593      0.001863         0.004494        0.000884   0.001   \n",
       "11        0.002534      0.001763         0.002337        0.000831   0.001   \n",
       "12        0.003225      0.000517         0.000971        0.000948   0.001   \n",
       "13        0.016480      0.001495         0.006443        0.001653   0.001   \n",
       "14        0.006035      0.001882         0.003141        0.000384   0.001   \n",
       "15        0.003890      0.002247         0.002678        0.000867   0.001   \n",
       "16        0.013078      0.000925         0.004604        0.001338   0.001   \n",
       "17        0.003382      0.001129         0.001726        0.000409   0.001   \n",
       "18        0.002674      0.001261         0.001461        0.000794   0.001   \n",
       "19        0.018918      0.001789         0.006062        0.002006   0.001   \n",
       "20        0.003905      0.000693         0.002731        0.000975   0.001   \n",
       "21        0.003668      0.000677         0.003367        0.000347    0.01   \n",
       "22        0.009298      0.001295         0.003168        0.001062    0.01   \n",
       "23        0.004971      0.001173         0.002635        0.001497    0.01   \n",
       "24        0.002285      0.001176         0.002245        0.001496    0.01   \n",
       "25        0.008414      0.001034         0.003054        0.000943    0.01   \n",
       "26        0.004225      0.001084         0.004130        0.001084    0.01   \n",
       "27        0.004236      0.001213         0.003737        0.001114    0.01   \n",
       "28        0.019970      0.002492         0.006957        0.000794    0.01   \n",
       "29        0.006798      0.000405         0.003189        0.000488    0.01   \n",
       "..             ...           ...              ...             ...     ...   \n",
       "117       0.001290      0.001637         0.001563        0.001914     100   \n",
       "118       0.014534      0.001471         0.004221        0.000606     100   \n",
       "119       0.002983      0.001072         0.002305        0.001493     100   \n",
       "120       0.002667      0.000741         0.002064        0.000760     100   \n",
       "121       0.020933      0.003826         0.004692        0.002151     100   \n",
       "122       0.003011      0.001520         0.001452        0.000888     100   \n",
       "123       0.002166      0.001571         0.002623        0.000810     100   \n",
       "124       0.020395      0.003564         0.005243        0.000209     100   \n",
       "125       0.006179      0.002385         0.002646        0.001642     100   \n",
       "126       0.002852      0.001604         0.004033        0.001315    1000   \n",
       "127       0.005549      0.002583         0.002818        0.000755    1000   \n",
       "128       0.004739      0.002383         0.002732        0.000392    1000   \n",
       "129       0.002785      0.000612         0.002462        0.000573    1000   \n",
       "130       0.005896      0.002137         0.002221        0.000453    1000   \n",
       "131       0.002604      0.000882         0.001405        0.000805    1000   \n",
       "132       0.002120      0.001019         0.001850        0.000915    1000   \n",
       "133       0.005362      0.000470         0.002838        0.000672    1000   \n",
       "134       0.002639      0.001599         0.001643        0.000999    1000   \n",
       "135       0.000708      0.000586         0.000953        0.000702    1000   \n",
       "136       0.005112      0.001695         0.001373        0.001327    1000   \n",
       "137       0.002440      0.001475         0.000611        0.001223    1000   \n",
       "138       0.000200      0.000400         0.002066        0.001861    1000   \n",
       "139       0.013809      0.002865         0.002911        0.000814    1000   \n",
       "140       0.001413      0.001501         0.001001        0.001512    1000   \n",
       "141       0.001294      0.001938         0.001754        0.001245    1000   \n",
       "142       0.014803      0.003577         0.002775        0.001690    1000   \n",
       "143       0.003373      0.000763         0.000400        0.000489    1000   \n",
       "144       0.000423      0.000519         0.001199        0.001448    1000   \n",
       "145       0.010885      0.001120         0.001629        0.001483    1000   \n",
       "146       0.002737      0.002007         0.001290        0.001448    1000   \n",
       "\n",
       "    param_gamma param_kernel  \\\n",
       "0         0.001       linear   \n",
       "1         0.001          rbf   \n",
       "2         0.001         poly   \n",
       "3          0.01       linear   \n",
       "4          0.01          rbf   \n",
       "5          0.01         poly   \n",
       "6           0.1       linear   \n",
       "7           0.1          rbf   \n",
       "8           0.1         poly   \n",
       "9             1       linear   \n",
       "10            1          rbf   \n",
       "11            1         poly   \n",
       "12           10       linear   \n",
       "13           10          rbf   \n",
       "14           10         poly   \n",
       "15          100       linear   \n",
       "16          100          rbf   \n",
       "17          100         poly   \n",
       "18         1000       linear   \n",
       "19         1000          rbf   \n",
       "20         1000         poly   \n",
       "21        0.001       linear   \n",
       "22        0.001          rbf   \n",
       "23        0.001         poly   \n",
       "24         0.01       linear   \n",
       "25         0.01          rbf   \n",
       "26         0.01         poly   \n",
       "27          0.1       linear   \n",
       "28          0.1          rbf   \n",
       "29          0.1         poly   \n",
       "..          ...          ...   \n",
       "117          10       linear   \n",
       "118          10          rbf   \n",
       "119          10         poly   \n",
       "120         100       linear   \n",
       "121         100          rbf   \n",
       "122         100         poly   \n",
       "123        1000       linear   \n",
       "124        1000          rbf   \n",
       "125        1000         poly   \n",
       "126       0.001       linear   \n",
       "127       0.001          rbf   \n",
       "128       0.001         poly   \n",
       "129        0.01       linear   \n",
       "130        0.01          rbf   \n",
       "131        0.01         poly   \n",
       "132         0.1       linear   \n",
       "133         0.1          rbf   \n",
       "134         0.1         poly   \n",
       "135           1       linear   \n",
       "136           1          rbf   \n",
       "137           1         poly   \n",
       "138          10       linear   \n",
       "139          10          rbf   \n",
       "140          10         poly   \n",
       "141         100       linear   \n",
       "142         100          rbf   \n",
       "143         100         poly   \n",
       "144        1000       linear   \n",
       "145        1000          rbf   \n",
       "146        1000         poly   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0    {'C': 0.001, 'gamma': 0.001, 'kernel': 'linear'}           0.898876   \n",
       "1       {'C': 0.001, 'gamma': 0.001, 'kernel': 'rbf'}           0.000000   \n",
       "2      {'C': 0.001, 'gamma': 0.001, 'kernel': 'poly'}           0.280702   \n",
       "3     {'C': 0.001, 'gamma': 0.01, 'kernel': 'linear'}           0.898876   \n",
       "4        {'C': 0.001, 'gamma': 0.01, 'kernel': 'rbf'}           0.000000   \n",
       "5       {'C': 0.001, 'gamma': 0.01, 'kernel': 'poly'}           0.911111   \n",
       "6      {'C': 0.001, 'gamma': 0.1, 'kernel': 'linear'}           0.898876   \n",
       "7         {'C': 0.001, 'gamma': 0.1, 'kernel': 'rbf'}           0.000000   \n",
       "8        {'C': 0.001, 'gamma': 0.1, 'kernel': 'poly'}           0.980000   \n",
       "9        {'C': 0.001, 'gamma': 1, 'kernel': 'linear'}           0.898876   \n",
       "10          {'C': 0.001, 'gamma': 1, 'kernel': 'rbf'}           0.000000   \n",
       "11         {'C': 0.001, 'gamma': 1, 'kernel': 'poly'}           1.000000   \n",
       "12      {'C': 0.001, 'gamma': 10, 'kernel': 'linear'}           0.898876   \n",
       "13         {'C': 0.001, 'gamma': 10, 'kernel': 'rbf'}           0.000000   \n",
       "14        {'C': 0.001, 'gamma': 10, 'kernel': 'poly'}           1.000000   \n",
       "15     {'C': 0.001, 'gamma': 100, 'kernel': 'linear'}           0.898876   \n",
       "16        {'C': 0.001, 'gamma': 100, 'kernel': 'rbf'}           0.000000   \n",
       "17       {'C': 0.001, 'gamma': 100, 'kernel': 'poly'}           1.000000   \n",
       "18    {'C': 0.001, 'gamma': 1000, 'kernel': 'linear'}           0.898876   \n",
       "19       {'C': 0.001, 'gamma': 1000, 'kernel': 'rbf'}           0.000000   \n",
       "20      {'C': 0.001, 'gamma': 1000, 'kernel': 'poly'}           1.000000   \n",
       "21    {'C': 0.01, 'gamma': 0.001, 'kernel': 'linear'}           0.934783   \n",
       "22       {'C': 0.01, 'gamma': 0.001, 'kernel': 'rbf'}           0.515152   \n",
       "23      {'C': 0.01, 'gamma': 0.001, 'kernel': 'poly'}           0.393443   \n",
       "24     {'C': 0.01, 'gamma': 0.01, 'kernel': 'linear'}           0.934783   \n",
       "25        {'C': 0.01, 'gamma': 0.01, 'kernel': 'rbf'}           0.873563   \n",
       "26       {'C': 0.01, 'gamma': 0.01, 'kernel': 'poly'}           0.934783   \n",
       "27      {'C': 0.01, 'gamma': 0.1, 'kernel': 'linear'}           0.934783   \n",
       "28         {'C': 0.01, 'gamma': 0.1, 'kernel': 'rbf'}           0.934783   \n",
       "29        {'C': 0.01, 'gamma': 0.1, 'kernel': 'poly'}           1.000000   \n",
       "..                                                ...                ...   \n",
       "117       {'C': 100, 'gamma': 10, 'kernel': 'linear'}           1.000000   \n",
       "118          {'C': 100, 'gamma': 10, 'kernel': 'rbf'}           0.989899   \n",
       "119         {'C': 100, 'gamma': 10, 'kernel': 'poly'}           1.000000   \n",
       "120      {'C': 100, 'gamma': 100, 'kernel': 'linear'}           1.000000   \n",
       "121         {'C': 100, 'gamma': 100, 'kernel': 'rbf'}           0.980000   \n",
       "122        {'C': 100, 'gamma': 100, 'kernel': 'poly'}           1.000000   \n",
       "123     {'C': 100, 'gamma': 1000, 'kernel': 'linear'}           1.000000   \n",
       "124        {'C': 100, 'gamma': 1000, 'kernel': 'rbf'}           0.899083   \n",
       "125       {'C': 100, 'gamma': 1000, 'kernel': 'poly'}           1.000000   \n",
       "126   {'C': 1000, 'gamma': 0.001, 'kernel': 'linear'}           1.000000   \n",
       "127      {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}           1.000000   \n",
       "128     {'C': 1000, 'gamma': 0.001, 'kernel': 'poly'}           0.980000   \n",
       "129    {'C': 1000, 'gamma': 0.01, 'kernel': 'linear'}           1.000000   \n",
       "130       {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}           1.000000   \n",
       "131      {'C': 1000, 'gamma': 0.01, 'kernel': 'poly'}           1.000000   \n",
       "132     {'C': 1000, 'gamma': 0.1, 'kernel': 'linear'}           1.000000   \n",
       "133        {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}           1.000000   \n",
       "134       {'C': 1000, 'gamma': 0.1, 'kernel': 'poly'}           1.000000   \n",
       "135       {'C': 1000, 'gamma': 1, 'kernel': 'linear'}           1.000000   \n",
       "136          {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}           1.000000   \n",
       "137         {'C': 1000, 'gamma': 1, 'kernel': 'poly'}           1.000000   \n",
       "138      {'C': 1000, 'gamma': 10, 'kernel': 'linear'}           1.000000   \n",
       "139         {'C': 1000, 'gamma': 10, 'kernel': 'rbf'}           0.989899   \n",
       "140        {'C': 1000, 'gamma': 10, 'kernel': 'poly'}           1.000000   \n",
       "141     {'C': 1000, 'gamma': 100, 'kernel': 'linear'}           1.000000   \n",
       "142        {'C': 1000, 'gamma': 100, 'kernel': 'rbf'}           0.980000   \n",
       "143       {'C': 1000, 'gamma': 100, 'kernel': 'poly'}           1.000000   \n",
       "144    {'C': 1000, 'gamma': 1000, 'kernel': 'linear'}           1.000000   \n",
       "145       {'C': 1000, 'gamma': 1000, 'kernel': 'rbf'}           0.899083   \n",
       "146      {'C': 1000, 'gamma': 1000, 'kernel': 'poly'}           1.000000   \n",
       "\n",
       "     split1_test_score  ...  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0             0.857143  ...         0.890442        0.026446              124   \n",
       "1             0.637681  ...         0.250082        0.305349              138   \n",
       "2             0.400000  ...         0.346326        0.050141              137   \n",
       "3             0.857143  ...         0.890442        0.026446              124   \n",
       "4             0.637681  ...         0.250082        0.305349              138   \n",
       "5             0.871795  ...         0.908812        0.028185              119   \n",
       "6             0.857143  ...         0.890442        0.026446              124   \n",
       "7             0.637681  ...         0.250082        0.305349              138   \n",
       "8             0.977778  ...         0.980200        0.009561               84   \n",
       "9             0.857143  ...         0.890442        0.026446              124   \n",
       "10            0.637681  ...         0.250082        0.305349              138   \n",
       "11            1.000000  ...         0.995553        0.005506               18   \n",
       "12            0.857143  ...         0.890442        0.026446              124   \n",
       "13            0.637681  ...         0.250082        0.305349              138   \n",
       "14            1.000000  ...         0.995553        0.005506               18   \n",
       "15            0.857143  ...         0.890442        0.026446              124   \n",
       "16            0.637681  ...         0.250082        0.305349              138   \n",
       "17            1.000000  ...         0.995553        0.005506               18   \n",
       "18            0.857143  ...         0.890442        0.026446              124   \n",
       "19            0.637681  ...         0.250082        0.305349              138   \n",
       "20            1.000000  ...         0.995553        0.005506               18   \n",
       "21            0.886076  ...         0.923151        0.032096              108   \n",
       "22            0.794521  ...         0.652529        0.152723              135   \n",
       "23            0.508475  ...         0.474383        0.042587              136   \n",
       "24            0.886076  ...         0.923151        0.032096              108   \n",
       "25            0.857143  ...         0.875778        0.014271              132   \n",
       "26            0.913580  ...         0.929599        0.021787              105   \n",
       "27            0.886076  ...         0.923151        0.032096              108   \n",
       "28            0.913580  ...         0.929475        0.021559              107   \n",
       "29            0.988764  ...         0.992844        0.009677               63   \n",
       "..                 ...  ...              ...             ...              ...   \n",
       "117           1.000000  ...         0.998032        0.003951                1   \n",
       "118           1.000000  ...         0.994075        0.007860               60   \n",
       "119           1.000000  ...         0.995553        0.005506               18   \n",
       "120           1.000000  ...         0.998032        0.003951                1   \n",
       "121           0.988764  ...         0.974540        0.020368               89   \n",
       "122           1.000000  ...         0.995553        0.005506               18   \n",
       "123           1.000000  ...         0.998032        0.003951                1   \n",
       "124           0.916667  ...         0.915487        0.033797              115   \n",
       "125           1.000000  ...         0.995553        0.005506               18   \n",
       "126           1.000000  ...         0.998032        0.003951                1   \n",
       "127           0.977778  ...         0.991149        0.008336               69   \n",
       "128           0.977778  ...         0.980200        0.009561               84   \n",
       "129           1.000000  ...         0.998032        0.003951                1   \n",
       "130           1.000000  ...         0.997580        0.004827               16   \n",
       "131           1.000000  ...         0.995553        0.005506               18   \n",
       "132           1.000000  ...         0.998032        0.003951                1   \n",
       "133           1.000000  ...         0.998032        0.003951                1   \n",
       "134           1.000000  ...         0.995553        0.005506               18   \n",
       "135           1.000000  ...         0.998032        0.003951                1   \n",
       "136           0.988764  ...         0.991367        0.007606               66   \n",
       "137           1.000000  ...         0.995553        0.005506               18   \n",
       "138           1.000000  ...         0.998032        0.003951                1   \n",
       "139           1.000000  ...         0.994075        0.007860               60   \n",
       "140           1.000000  ...         0.995553        0.005506               18   \n",
       "141           1.000000  ...         0.998032        0.003951                1   \n",
       "142           0.988764  ...         0.974540        0.020368               89   \n",
       "143           1.000000  ...         0.995553        0.005506               18   \n",
       "144           1.000000  ...         0.998032        0.003951                1   \n",
       "145           0.916667  ...         0.915487        0.033797              115   \n",
       "146           1.000000  ...         0.995553        0.005506               18   \n",
       "\n",
       "     split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0              0.890909            0.903790            0.899135   \n",
       "1              0.000000            0.669039            0.676106   \n",
       "2              0.366071            0.336283            0.360515   \n",
       "3              0.890909            0.903790            0.899135   \n",
       "4              0.000000            0.669039            0.676106   \n",
       "5              0.907463            0.916427            0.917847   \n",
       "6              0.890909            0.903790            0.899135   \n",
       "7              0.000000            0.669039            0.676106   \n",
       "8              0.989189            0.992084            0.987080   \n",
       "9              0.890909            0.903790            0.899135   \n",
       "10             0.000000            0.669039            0.676106   \n",
       "11             1.000000            1.000000            1.000000   \n",
       "12             0.890909            0.903790            0.899135   \n",
       "13             0.000000            0.669039            0.676106   \n",
       "14             1.000000            1.000000            1.000000   \n",
       "15             0.890909            0.903790            0.899135   \n",
       "16             0.000000            0.669039            0.676106   \n",
       "17             1.000000            1.000000            1.000000   \n",
       "18             0.890909            0.903790            0.899135   \n",
       "19             0.000000            0.669039            0.676106   \n",
       "20             1.000000            1.000000            1.000000   \n",
       "21             0.923529            0.931818            0.929972   \n",
       "22             0.547619            0.810127            0.879765   \n",
       "23             0.487603            0.477733            0.472000   \n",
       "24             0.923529            0.931818            0.929972   \n",
       "25             0.870370            0.903790            0.899135   \n",
       "26             0.929825            0.934844            0.936288   \n",
       "27             0.923529            0.931818            0.929972   \n",
       "28             0.923529            0.932203            0.930748   \n",
       "29             0.994565            0.997347            1.000000   \n",
       "..                  ...                 ...                 ...   \n",
       "117            1.000000            1.000000            1.000000   \n",
       "118            1.000000            1.000000            1.000000   \n",
       "119            1.000000            1.000000            1.000000   \n",
       "120            1.000000            1.000000            1.000000   \n",
       "121            1.000000            1.000000            1.000000   \n",
       "122            1.000000            1.000000            1.000000   \n",
       "123            1.000000            1.000000            1.000000   \n",
       "124            1.000000            1.000000            1.000000   \n",
       "125            1.000000            1.000000            1.000000   \n",
       "126            1.000000            1.000000            1.000000   \n",
       "127            0.991870            0.994709            0.989637   \n",
       "128            0.989189            0.992084            0.987080   \n",
       "129            1.000000            1.000000            1.000000   \n",
       "130            0.997275            0.997347            1.000000   \n",
       "131            1.000000            1.000000            1.000000   \n",
       "132            1.000000            1.000000            1.000000   \n",
       "133            1.000000            1.000000            1.000000   \n",
       "134            1.000000            1.000000            1.000000   \n",
       "135            1.000000            1.000000            1.000000   \n",
       "136            1.000000            1.000000            1.000000   \n",
       "137            1.000000            1.000000            1.000000   \n",
       "138            1.000000            1.000000            1.000000   \n",
       "139            1.000000            1.000000            1.000000   \n",
       "140            1.000000            1.000000            1.000000   \n",
       "141            1.000000            1.000000            1.000000   \n",
       "142            1.000000            1.000000            1.000000   \n",
       "143            1.000000            1.000000            1.000000   \n",
       "144            1.000000            1.000000            1.000000   \n",
       "145            1.000000            1.000000            1.000000   \n",
       "146            1.000000            1.000000            1.000000   \n",
       "\n",
       "     split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "0              0.898204            0.880000          0.894408         0.008302  \n",
       "1              0.000000            0.000000          0.269029         0.329500  \n",
       "2              0.342342            0.345455          0.350133         0.011280  \n",
       "3              0.898204            0.880000          0.894408         0.008302  \n",
       "4              0.000000            0.000000          0.269029         0.329500  \n",
       "5              0.914454            0.896970          0.910632         0.007706  \n",
       "6              0.898204            0.880000          0.894408         0.008302  \n",
       "7              0.000000            0.000000          0.269029         0.329500  \n",
       "8              0.994595            0.989130          0.990416         0.002628  \n",
       "9              0.898204            0.880000          0.894408         0.008302  \n",
       "10             0.000000            0.000000          0.269029         0.329500  \n",
       "11             1.000000            1.000000          1.000000         0.000000  \n",
       "12             0.898204            0.880000          0.894408         0.008302  \n",
       "13             0.000000            0.000000          0.269029         0.329500  \n",
       "14             1.000000            1.000000          1.000000         0.000000  \n",
       "15             0.898204            0.880000          0.894408         0.008302  \n",
       "16             0.000000            0.000000          0.269029         0.329500  \n",
       "17             1.000000            1.000000          1.000000         0.000000  \n",
       "18             0.898204            0.880000          0.894408         0.008302  \n",
       "19             0.000000            0.000000          0.269029         0.329500  \n",
       "20             1.000000            1.000000          1.000000         0.000000  \n",
       "21             0.933333            0.913433          0.926417         0.007302  \n",
       "22             0.573643            0.495868          0.661404         0.153526  \n",
       "23             0.485597            0.457627          0.476112         0.010800  \n",
       "24             0.933333            0.913433          0.926417         0.007302  \n",
       "25             0.871166            0.858934          0.880679         0.017575  \n",
       "26             0.933718            0.932551          0.933445         0.002192  \n",
       "27             0.933333            0.913433          0.926417         0.007302  \n",
       "28             0.930233            0.913433          0.926029         0.006971  \n",
       "29             0.997290            0.997260          0.997293         0.001719  \n",
       "..                  ...                 ...               ...              ...  \n",
       "117            1.000000            1.000000          1.000000         0.000000  \n",
       "118            1.000000            1.000000          1.000000         0.000000  \n",
       "119            1.000000            1.000000          1.000000         0.000000  \n",
       "120            1.000000            1.000000          1.000000         0.000000  \n",
       "121            1.000000            1.000000          1.000000         0.000000  \n",
       "122            1.000000            1.000000          1.000000         0.000000  \n",
       "123            1.000000            1.000000          1.000000         0.000000  \n",
       "124            1.000000            1.000000          1.000000         0.000000  \n",
       "125            1.000000            1.000000          1.000000         0.000000  \n",
       "126            1.000000            1.000000          1.000000         0.000000  \n",
       "127            0.994595            0.991826          0.992527         0.001914  \n",
       "128            0.994595            0.989130          0.990416         0.002628  \n",
       "129            1.000000            1.000000          1.000000         0.000000  \n",
       "130            0.997290            0.997260          0.997835         0.001083  \n",
       "131            1.000000            1.000000          1.000000         0.000000  \n",
       "132            1.000000            1.000000          1.000000         0.000000  \n",
       "133            1.000000            1.000000          1.000000         0.000000  \n",
       "134            1.000000            1.000000          1.000000         0.000000  \n",
       "135            1.000000            1.000000          1.000000         0.000000  \n",
       "136            1.000000            1.000000          1.000000         0.000000  \n",
       "137            1.000000            1.000000          1.000000         0.000000  \n",
       "138            1.000000            1.000000          1.000000         0.000000  \n",
       "139            1.000000            1.000000          1.000000         0.000000  \n",
       "140            1.000000            1.000000          1.000000         0.000000  \n",
       "141            1.000000            1.000000          1.000000         0.000000  \n",
       "142            1.000000            1.000000          1.000000         0.000000  \n",
       "143            1.000000            1.000000          1.000000         0.000000  \n",
       "144            1.000000            1.000000          1.000000         0.000000  \n",
       "145            1.000000            1.000000          1.000000         0.000000  \n",
       "146            1.000000            1.000000          1.000000         0.000000  \n",
       "\n",
       "[147 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        82\n",
      "           1       0.90      1.00      0.95        75\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       157\n",
      "   macro avg       0.95      0.95      0.95       157\n",
      "weighted avg       0.95      0.95      0.95       157\n",
      "\n",
      "Accuracy: 0.9490\n"
     ]
    }
   ],
   "source": [
    "ksvc_rbf = SVC(kernel = 'rbf', C = 0.1, gamma = 10)\n",
    "ksvc_rbf.fit(X_trainval,y_cls_trainval)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_cls_test, ksvc_rbf.predict(X_test)))\n",
    "print('Accuracy: {:.4f}'.format(ksvc_rbf.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelized Support Vector Machine without PCA\n",
    "\n",
    "The Mean Train F1-score is  1.0000\n",
    "\n",
    "The Mean Validation F1-score is 1.0000\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.98      0.99        60\n",
    "           1       0.98      1.00      0.99        65\n",
    "           \n",
    "Accuracy: 0.9920"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': range(1, 10)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid = {'max_depth': range(1, 10)}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state = 0), param_grid, cv=kfold, return_train_score=True, scoring='f1')\n",
    "grid_search.fit(X_trainval, y_cls_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 4}\n",
      "Best Mean Train F1-score: 0.9973\n",
      "Best Mean Validation F1-score: 0.9806\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best Mean Train F1-score: {:.4f}\".format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\n",
    "print(\"Best Mean Validation F1-score: {:.4f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96        82\n",
      "           1       0.94      0.97      0.95        75\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       157\n",
      "   macro avg       0.96      0.96      0.96       157\n",
      "weighted avg       0.96      0.96      0.96       157\n",
      "\n",
      "Accuracy: 0.9554\n"
     ]
    }
   ],
   "source": [
    "dtree = DecisionTreeClassifier(random_state = 0, max_depth = 2)\n",
    "dtree.fit(X_trainval, y_cls_trainval)\n",
    "\n",
    "print(classification_report(y_cls_test, dtree.predict(X_test)))\n",
    "print('Accuracy: {:.4f}'.format(dtree.score(X_test, y_cls_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier without PCA\n",
    "\n",
    "The Mean Train F1-score is 1.0000\n",
    "\n",
    "The Mean Validation F1-score is 0.9976\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        60\n",
    "           1       1.00      1.00      1.00        65\n",
    "\n",
    "Accuracy: 1.0000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
